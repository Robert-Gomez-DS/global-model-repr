{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Dissection\n",
    "\n",
    "Network dissection is a systematic method for finding single units (filters, or neurons) that match meaningful semantic concepts in a vision network, and for quantifying the closeness of the match.\n",
    "\n",
    "In this notebook we will use network dissection to explore the neurons in an image classification network.\n",
    "\n",
    "Our fundamental question is this: how does the network decompose the task of understanding that an image is a baseball field?  Does it identify any features that are understandable to a human?\n",
    "\n",
    "Simply running this notebook will provide a simple dissection, but at each step, there are exercises for modifying the notebook to find more interesting results.\n",
    "\n",
    "## About the netdissect library\n",
    "\n",
    "The netdissect library contains several useful packages for inspecting internals of a vision network.\n",
    "Here are packages that we use in this notebook:\n",
    "\n",
    " * **nethook** wraps any pytorch model, adding the ability to record or modify any internal computation.\n",
    " * **imgviz** provides ImageVisualizer, that collects together several useful image visualization functions.\n",
    " * **show** arranges nested arrays of PIL images and strings as nicely formatted HTML for display in a notebook.\n",
    " * **segmenter** provides an interface and a pretrained implementation for a semantic segmentation network.\n",
    " * **tally** gathers statistics over a dataset, based on your function to compute features for each datum.\n",
    " * **renormalize** deals with conversions between the zoo of RGB encoding scales typically seen in vision data.\n",
    " * **upsample** provids simple functions for resampling grid data at higher or lower resolutions.\n",
    " * **pbar** is a progress bar.\n",
    "\n",
    "These will be explained a bit more in the exercises below.  Of course you can always run `help(object)` for a bit more information on most things in the library.\n",
    "\n",
    "In addition, for this tutorial we have a package **settting**, which automatically downloads and creates datasets and pretrained models that we will be looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, matplotlib.pyplot as plt\n",
    "from netdissect import nethook, imgviz, show, segmenter, renormalize, upsample, tally, pbar\n",
    "from netdissect import setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained models and data\n",
    "\n",
    "Here are some fixed variables that we define up-front for all the objects that we will be inspecting in this tutorial.\n",
    "\n",
    "* **model** is the network we will look at.  It is a VGG convolutional network, trained to classify images of scenes into one of 365 place categories.  We wrap `model` as a `nethook.InstrumenteModel` so that we can easily retrieve and modiry its internal activations.\n",
    "* **ds** is a small held-out sample from the Places dataset that was used to train the model; each entry is a pytorch tensor representing an image, and an integer representing the class.  A pytorch dataset can be derefernces like an array, so `ds[35]` is a pair `(x, y)` where `x` is a tensor containing RGB image data for a scene and `y` is an integer for the human-given class label.  Classnames are available as `ds.class[y]`.\n",
    "* **renorm** is a function that renormalizes RGB data from the staistically-based scaling used in `ds` to a simple `[-1...1]` range scale.\n",
    "* **segmodel** is a semantic segmentation network trained to recognize a large vocabulary of objects and parts of objects within scenes.  We will use it as a reference, to see if there are any internal filters that approximately match the same concepts.\n",
    "* **seglabels** are human-readable names for the numerical segmentation classes.\n",
    "* **iv** is an image visualization object that visualizes 2d data such as images and heatmaps as 224x224 images.\n",
    "* **ivsmall** is another visualization object, but outputs smaller 56x56 images.\n",
    "* **resfile** is a function that generates filenames in a results subdirectory that we will use for caching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setting.load_vgg16()\n",
    "model = nethook.InstrumentedModel(model)\n",
    "model.cuda()\n",
    "ds = setting.load_dataset('places', 'val')\n",
    "renorm = renormalize.renormalizer(ds, target='zc')\n",
    "segmodel, seglabels, segcatlabels = setting.load_segmenter('netpqc')\n",
    "iv = imgviz.ImageVisualizer(224, source=ds, percent_level=0.99)\n",
    "ivsmall = imgviz.ImageVisualizer((56, 56), source=ds, percent_level=0.99)\n",
    "def resfile(f):\n",
    "    return os.path.join('results/vgg16-places', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: test the model\n",
    "\n",
    "When the model is run on a batch of images, it e.\n",
    "\n",
    "In the short example below:\n",
    "* **indexes** is a list of dataset indexes to retrieve.  `i` indicates a dataset index, and `j` is an index into the indexes array.\n",
    "* **batch** is a `12 x 3 x 224 x 224` tensor that stacks up twelve RGB 224x224 images from the dataset.\n",
    "* When we run `model(batch.cuda())`, it scores every image for every class, making a `12 x 365` tensor of scores.\n",
    "* Then `.max(dim=1)` finds the maximum of 365 scores for each image; it returns a (scores, indexes) tuple.\n",
    "* **preds** is a tensor of 12 highest scoring class indexes (each one a number up to 365) predicted by the model.\n",
    "* `iv.image(batch[j])` turns the jth `3 x 224 x 224` tensor into a PIL image for display.\n",
    "* `ds.classes[ds[i][1]]` shows the human ground-truth label for the `i`th image in the dataset.\n",
    "\n",
    "So the loop shows a set of twelve images, each with the dataset label and the model prediction.\n",
    "\n",
    "Scene classification is difficult and sometimes ambiguous; nevertheless the model does reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = range(100, 112)\n",
    "batch = torch.stack([ds[i][0] for i in indexes])\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "show([[\n",
    "    iv.image(batch[j]),\n",
    "    'label: ' + ds.classes[ds[i][1]],\n",
    "    'pred: ' + ds.classes[preds[j]]]\n",
    "    for j, i in enumerate(indexes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: measure accuracy\n",
    "\n",
    "Fix the loop below to measure accuracy of the model on a sample of 2000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "tested = 0\n",
    "for imagebatch, labelbatch in pbar(torch.utils.data.DataLoader(ds, batch_size=100)):\n",
    "    modelpreds = model(imagebatch.cuda()).max(1)[1]\n",
    "    correct += 0 # fixme\n",
    "    tested += len(labelbatch)\n",
    "    if tested >= 2000:\n",
    "        break\n",
    "print('%d correct out of %d' % (correct, tested))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Examine raw unit activations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layername = 'features.conv5_1'\n",
    "model.retain_layer(layername)\n",
    "model(batch.cuda())\n",
    "acts = model.retained_layer(layername).cpu()\n",
    "show([\n",
    "    [\n",
    "        [ivsmall.masked_image(batch[0], acts[0], u)],\n",
    "        [ivsmall.heatmap(acts[0], u, mode='nearest')],\n",
    "        'unit %d' % u\n",
    "    ]\n",
    "    for u in range(min(21, acts.shape[1]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upfn = upsample.upsampler(\n",
    "    target_shape=(56, 56),\n",
    "    data_shape=(7, 7),\n",
    ")\n",
    "\n",
    "def flatten_activations(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "\n",
    "rq = tally.tally_quantile(\n",
    "    flatten_activations,\n",
    "    dataset=ds,\n",
    "    sample_size=1000,\n",
    "    batch_size=100,\n",
    "    cachefile=resfile(layername + '_rq.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rq.quantiles(0.9))\n",
    "\n",
    "# Which unit is activating more often than the others?\n",
    "rq.quantiles(0.9).max(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = segmodel.segment_batch(renorm(batch).cuda(), downsample=4)\n",
    "show([(iv.image(batch[i]), iv.segmentation(seg[i, 0]),\n",
    "            iv.segment_key(seg[i,0], segmodel))\n",
    "            for i in range(len(seg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "def max_activations(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    return acts.view(acts.shape[:2] + (-1,)).max(2)[0]\n",
    "\n",
    "def mean_activations(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    return acts.view(acts.shape[:2] + (-1,)).mean(2)\n",
    "\n",
    "topk = tally.tally_topk(\n",
    "    mean_activations,\n",
    "    dataset=ds,\n",
    "    sample_size=sample_size,\n",
    "    batch_size=100,\n",
    "    cachefile=resfile(layername + '_mean_topk.npz')\n",
    ")\n",
    "\n",
    "top_indexes = topk.result()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show.blocks([\n",
    "    ['unit %d' % u,\n",
    "     'img %d' % i,\n",
    "     'pred: %s' % ds.classes[model(ds[i][0][None].cuda()).max(1)[1].item()],\n",
    "     [iv.masked_image(\n",
    "        ds[i][0],\n",
    "        model.retained_layer(layername)[0],\n",
    "        u)]\n",
    "    ]\n",
    "    for u in [12]\n",
    "    for i in top_indexes[u, :20]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activations(image_batch):\n",
    "    image_batch = image_batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts_batch = model.retained_layer(layername)\n",
    "    return acts_batch\n",
    "\n",
    "unit_images = iv.masked_images_for_topk(\n",
    "    compute_activations,\n",
    "    ds,\n",
    "    topk,\n",
    "    k=5,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    cachefile=resfile(layername + '_top10images.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_at_99 = rq.quantiles(0.99).cuda()[None,:,None,None]\n",
    "\n",
    "def compute_selected_segments(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    iacts = (hacts > level_at_99).float() # indicator where > 0.99 percentile.\n",
    "    return tally.conditional_samples(iacts, seg)\n",
    "\n",
    "condi99 = tally.tally_conditional_mean(\n",
    "    compute_selected_segments,\n",
    "    dataset=ds,\n",
    "    sample_size=sample_size,\n",
    "    cachefile=resfile(layername + '_condi99.npz'))\n",
    "\n",
    "iou99 = tally.iou_from_conditional_indicator_mean(condi99)\n",
    "iou99.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_unit_label_99 = sorted([(\n",
    "    unit, concept.item(), seglabels[concept], bestiou.item())\n",
    "    for unit, (bestiou, concept) in enumerate(zip(*iou99.max(0)))],\n",
    "    key=lambda x: -x[-1])\n",
    "for unit, concept, label, score in iou_unit_label_99[:20]:\n",
    "    show(['unit %d; iou %g; label \"%s\"' % (unit, score, label),\n",
    "          [unit_images[unit]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold = 0.04\n",
    "unit_label_99 = [\n",
    "        (concept.item(), seglabels[concept],\n",
    "            segcatlabels[concept], bestiou.item())\n",
    "        for (bestiou, concept) in zip(*iou99.max(0))]\n",
    "labelcat_list = [labelcat\n",
    "        for concept, label, labelcat, iou in unit_label_99\n",
    "        if iou > iou_threshold]\n",
    "import IPython\n",
    "IPython.display.SVG(setting.graph_conceptcatlist(labelcat_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}