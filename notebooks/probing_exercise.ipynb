{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "probing_exercise.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkzRe66H3iVW",
        "colab_type": "code",
        "outputId": "b9a430a8-7eff-4475-a2a5-55138e56ccc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        }
      },
      "source": [
        "!git clone https://github.com/SIDN-IAP/global-model-repr\n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import sys\n",
        "sys.path.append('global-model-repr/')\n",
        "from probing.utils import get_sentence_repr, get_model_and_tokenizer, get_pos_data\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(\"device:\", device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'global-model-repr'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 176 (delta 57), reused 147 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (176/176), 1.15 MiB | 2.69 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=0a14070e6ad650787dc17ffcf423ad05d754bddd9b6d6abcaa3c67df0fce4ada\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxaHPQIG3iVf",
        "colab_type": "text"
      },
      "source": [
        "# Get data for part-of-speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-xo6-q3iVg",
        "colab_type": "code",
        "outputId": "f2095d5f-5504-4b72-b36e-c459f7e43859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "train_sentences, train_labels, test_sentences, test_labels, _, _, label2index = get_pos_data(\"global-model-repr/probing\", frac=0.01)\n",
        "num_labels = len(label2index)\n",
        "print(\"Training sentences:\", len(train_sentences), \"Test sentences:\", len(test_sentences))\n",
        "print(\"Unique labels:\", num_labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training sentences: 125 Test sentences: 21\n",
            "Unique labels: 15\n",
            "[tensor([ 9,  5,  9,  5,  7, 11,  4,  9,  9,  9,  5,  9,  5,  0, 11,  2,  0, 11,\n",
            "         2,  0, 11,  2,  9,  5,  2,  0,  7, 11,  5]), tensor([ 5,  0, 11,  2,  0,  7, 11, 12, 12,  4, 13, 11,  2, 11, 10,  4,  5,  5]), tensor([ 9,  5,  7, 11,  4,  8, 13, 12,  4,  2,  1,  7, 11,  4,  2,  9,  5]), tensor([ 1,  2, 13, 12, 12,  4,  2,  1, 11,  2,  0,  9,  2,  0,  9,  5]), tensor([ 0,  9,  2,  9, 12,  7,  2,  0,  9,  9,  5,  3, 13, 12,  4,  8,  4,  9,\n",
            "         9,  9,  3,  4,  2,  0,  7, 11, 11,  2,  0,  9, 11,  3,  2,  0, 11,  5]), tensor([ 0,  7, 12, 12,  4,  2,  0, 11,  2,  0, 11, 11,  5]), tensor([13,  4,  8, 13, 12,  4,  0, 11,  2, 13, 11, 11,  5]), tensor([ 0, 11, 12,  4,  2,  0,  9, 14,  9,  5,  9, 11,  2,  0, 11,  5]), tensor([ 8,  0, 11, 12,  3,  4, 10,  4, 11,  8,  4, 14,  4,  2, 11, 11,  5, 13,\n",
            "        12, 10,  4,  0, 11,  8,  0,  9,  4, 10,  4,  0,  7, 11,  3,  7,  5]), tensor([13,  4, 13,  3,  3,  2,  0,  9, 11, 13, 12,  3,  4,  2,  0,  7, 11,  2,\n",
            "         9,  5]), tensor([ 9,  5,  9,  5, 11,  4,  0, 11,  2,  0,  9,  9,  9,  8,  4, 13,  2,  9,\n",
            "         5]), tensor([ 0, 11, 11,  2,  9,  9,  4,  8, 11, 12,  4,  1, 11,  2,  9,  5,  8, 13,\n",
            "        12,  4,  0,  7, 11,  2,  0, 11,  2, 11,  5]), tensor([ 2,  9, 11, 12,  4,  0,  7, 11, 14, 13, 11,  8, 13, 12,  2, 13, 11,  3,\n",
            "         5]), tensor([ 2,  9,  5,  9,  9,  5,  1, 11, 11, 12,  4,  8, 13, 12, 12,  4,  5]), tensor([ 5,  9, 12,  2,  7,  9, 14,  4,  0,  3,  7, 11,  5, 13, 12,  2,  0, 11,\n",
            "         2,  9, 11, 14,  3,  4, 13,  2,  5]), tensor([ 0, 11, 12,  0,  7, 11,  2,  0, 11,  2, 11,  2,  0, 11,  8,  4,  2,  0,\n",
            "        11,  5, 11,  2,  0, 11,  5,  5]), tensor([ 2,  9,  9,  9,  9,  5,  7, 11,  2,  0,  9,  2,  9,  2,  9,  9,  5, 12,\n",
            "         4,  2, 13, 11,  2,  0,  9, 11,  5]), tensor([11,  4,  0, 11,  5,  9,  9,  5,  2,  9,  5]), tensor([13,  3,  4,  9,  9,  9,  5,  0, 11, 11,  2,  0, 11,  2,  0,  9, 11,  2,\n",
            "         9,  5]), tensor([13, 11, 12,  3,  4,  2,  0, 11,  5]), tensor([ 1,  7,  9, 11, 12, 12,  4,  2,  0,  7, 11, 14,  0, 11,  5])]\n",
            "[['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']'], ['DPA', ':', 'Iraqi', 'authorities', 'announced', 'that', 'they', 'had', 'busted', 'up', '3', 'terrorist', 'cells', 'operating', 'in', 'Baghdad', '.'], ['Two', 'of', 'them', 'were', 'being', 'run', 'by', '2', 'officials', 'of', 'the', 'Ministry', 'of', 'the', 'Interior', '!'], ['The', 'MoI', 'in', 'Iraq', 'is', 'equivalent', 'to', 'the', 'US', 'FBI', ',', 'so', 'this', 'would', 'be', 'like', 'having', 'J.', 'Edgar', 'Hoover', 'unwittingly', 'employ', 'at', 'a', 'high', 'level', 'members', 'of', 'the', 'Weathermen', 'bombers', 'back', 'in', 'the', '1960s', '.'], ['The', 'third', 'was', 'being', 'run', 'by', 'the', 'head', 'of', 'an', 'investment', 'firm', '.'], ['You', 'wonder', 'if', 'he', 'was', 'manipulating', 'the', 'market', 'with', 'his', 'bombing', 'targets', '.'], ['The', 'cells', 'were', 'operating', 'in', 'the', 'Ghazaliyah', 'and', 'al', '-', 'Jihad', 'districts', 'of', 'the', 'capital', '.'], ['Although', 'the', 'announcement', 'was', 'probably', 'made', 'to', 'show', 'progress', 'in', 'identifying', 'and', 'breaking', 'up', 'terror', 'cells', ',', 'I', 'do', \"n't\", 'find', 'the', 'news', 'that', 'the', 'Baathists', 'continue', 'to', 'penetrate', 'the', 'Iraqi', 'government', 'very', 'hopeful', '.'], ['It', 'reminds', 'me', 'too', 'much', 'of', 'the', 'ARVN', 'officers', 'who', 'were', 'secretly', 'working', 'for', 'the', 'other', 'side', 'in', 'Vietnam', '.'], ['Al', '-', 'Zaman', ':', 'Guerrillas', 'killed', 'a', 'member', 'of', 'the', 'Kurdistan', 'Democratic', 'Party', 'after', 'kidnapping', 'him', 'in', 'Mosul', '.'], ['The', 'police', 'commander', 'of', 'Ninevah', 'Province', 'announced', 'that', 'bombings', 'had', 'declined', '80', 'percent', 'in', 'Mosul', ',', 'whereas', 'there', 'had', 'been', 'a', 'big', 'jump', 'in', 'the', 'number', 'of', 'kidnappings', '.'], ['On', 'Wednesday', 'guerrillas', 'had', 'kidnapped', 'a', 'cosmetic', 'surgeon', 'and', 'his', 'wife', 'while', 'they', 'were', 'on', 'their', 'way', 'home', '.'], ['In', 'Suwayrah', ',', 'Kut', 'Province', ',', 'two', 'car', 'bombs', 'were', 'discovered', 'before', 'they', 'could', 'be', 'detonated', '.'], ['(', 'Kut', 'is', 'in', 'southeastern', 'Iraq', 'and', 'has', 'an', 'overwhelmingly', 'Shiite', 'population', ',', 'who', 'are', 'on', 'the', 'lookout', 'for', 'Baathist', 'saboteurs', 'and', 'willingly', 'turn', 'them', 'in', '.'], ['This', 'willingness', 'is', 'the', 'main', 'difference', 'in', 'the', 'number', 'of', 'bombings', 'in', 'the', 'south', 'as', 'opposed', 'to', 'the', 'center', '-', 'north', 'of', 'the', 'country', '.', ')'], ['In', 'Baghdad', 'Kadhim', 'Talal', 'Husain', ',', 'assistant', 'dean', 'at', 'the', 'School', 'of', 'Education', 'at', 'Mustansiriyah', 'University', ',', 'was', 'assassinated', 'with', 'his', 'driver', 'in', 'the', 'Salikh', 'district', '.'], ['Guerrillas', 'killed', 'an', 'engineer', ',', 'Asi', 'Ali', ',', 'from', 'Tikrit', '.'], ['They', 'also', 'killed', 'Shaikh', 'Hamid', \"'Akkab\", ',', 'a', 'clan', 'elder', 'of', 'a', 'branch', 'of', 'the', 'Dulaim', 'tribe', 'in', 'Tikrit', '.'], ['His', 'mother', 'was', 'also', 'killed', 'in', 'the', 'attack', '.'], ['Two', 'other', 'Dulaim', 'leaders', 'have', 'been', 'killed', 'in', 'the', 'past', 'week', 'and', 'a', 'half', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keg_tLaD3iVk",
        "colab_type": "text"
      },
      "source": [
        "# Set up model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXSLHwPJ3iVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        \n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        output = self.linear(input)\n",
        "        return output\n",
        "    \n",
        "\n",
        "class NonlinearClassifier(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(NonlinearClassifier, self).__init__()\n",
        "        \n",
        "        self.input2hidden = torch.nn.Linear(input_dim, input_dim)\n",
        "        self.hidden2output = torch.nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        hidden = self.input2hidden(input)\n",
        "        output = self.hidden2output(hidden)\n",
        "        return output\n",
        "    \n",
        "    \n",
        "def build_classifier(emb_dim, num_labels, nonlinear=False):\n",
        "\n",
        "    if nonlinear:\n",
        "        classifier = NonlinearClassifier(emb_dim, num_labels)\n",
        "    else:\n",
        "        classifier = Classifier(emb_dim, num_labels)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(classifier.parameters())\n",
        "\n",
        "    return classifier, criterion, optimizer\n",
        "\n",
        "\n",
        "model_name = 'bert-base-cased'\n",
        "# get model and tokenizer from Transformers\n",
        "model, tokenizer, sep, emb_dim = get_model_and_tokenizer(model_name, device)\n",
        "# build classifier\n",
        "classifier, criterion, optimizer = build_classifier(emb_dim, num_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuLxynuc3iVp",
        "colab_type": "code",
        "outputId": "9e2a7367-e2e4-41f6-d7e7-c67161b7633e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpNO3Ida3iVs",
        "colab_type": "code",
        "outputId": "97758698-c925-45aa-a4ba-b46e5ca0c4bf",
        "colab": {}
      },
      "source": [
        "print(classifier)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (linear): Linear(in_features=768, out_features=15, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KluH35v3iVv",
        "colab_type": "text"
      },
      "source": [
        "# Train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q_On99u3iVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_epochs, train_sentences, train_labels, \n",
        "          model, tokenizer, sep, model_name, device, \n",
        "          classifier, criterion, optimizer, layer=-1):\n",
        "    \n",
        "    num_total = sum([len(l) for l in train_labels])\n",
        "    for i in range(num_epochs):\n",
        "        total_loss = 0.\n",
        "        num_correct = 0.\n",
        "        for sentence, labels in zip(train_sentences, train_labels):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            sentence_repr = get_sentence_repr(sentence, model, tokenizer, sep, model_name, device)\n",
        "            # take layer representations\n",
        "            sentence_repr = sentence_repr[layer]\n",
        "            loss = 0\n",
        "            for word_repr, label in zip(sentence_repr, labels):\n",
        "                out = classifier(word_repr)\n",
        "                # we'll just just a batch of size 1 for simplicity \n",
        "                out = torch.unsqueeze(out, 0)\n",
        "                pred = out.max(1)[1]\n",
        "                if pred == label.item():\n",
        "                    num_correct += 1\n",
        "                loss += criterion(out, label.unsqueeze(0))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "#         print('Training epoch: {}, loss: {}, accuracy: {}'.format(i, total_loss/num_total, num_correct/num_total))\n",
        "    return total_loss/num_total, num_correct/num_total\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8Rge5S3iV0",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi0fG0vg3iV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(test_sentences, test_labels, \n",
        "             model, tokenizer, sep, model_name, device, \n",
        "             classifier, criterion, layer=-1):\n",
        "    \n",
        "    num_correct = 0.\n",
        "    num_total = sum([len(l) for l in test_labels])\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for sentence, labels in zip(test_sentences, test_labels):\n",
        "            sentence_repr = get_sentence_repr(sentence, model, tokenizer, sep, model_name, device)\n",
        "            sentence_repr = sentence_repr[layer]\n",
        "            for word_repr, label in zip(sentence_repr, labels):\n",
        "                out = classifier(word_repr)\n",
        "                out = torch.unsqueeze(out, 0)\n",
        "                pred = out.max(1)[1]\n",
        "                if pred == label:\n",
        "                    num_correct += 1\n",
        "                total_loss += criterion(out, label.unsqueeze(0))\n",
        "\n",
        "#     print('Testing loss: {}, accuracy: {}'.format(total_loss/num_total, num_correct/num_total))\n",
        "    return total_loss/num_total, num_correct/num_total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIBpbUIv3iV4",
        "colab_type": "text"
      },
      "source": [
        "# Experiment 1: Evaluate representation for POS quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Hsz50R3iV5",
        "colab_type": "code",
        "outputId": "311dd475-ee05-4d35-ffa6-92a084d8d7bc",
        "colab": {}
      },
      "source": [
        "train_loss, train_accuracy = train(2, train_sentences, train_labels, \n",
        "          model, tokenizer, sep, model_name, device, \n",
        "          classifier, criterion, optimizer)\n",
        "test_loss, test_accuracy = evaluate(test_sentences, test_labels, \n",
        "         model, tokenizer, sep, model_name, device, \n",
        "         classifier, criterion)\n",
        "print(\"Train accuracy: {}, Test accuracy: {}\".format(train_accuracy, test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.738976377952756, Test accuracy: 0.8425925925925926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RncHqDF3iV8",
        "colab_type": "text"
      },
      "source": [
        "# Experiment 2: Compare representation quality across layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYqr1lX93iV9",
        "colab_type": "code",
        "outputId": "765070f8-d1c8-47f1-a97a-9c8710f14c0f",
        "colab": {}
      },
      "source": [
        "num_layers = 12\n",
        "for l in range(num_layers):\n",
        "    classifier, criterion, optimizer = build_classifier(emb_dim, num_labels)\n",
        "    train_loss, train_accuracy = train(2, train_sentences, train_labels, \n",
        "          model, tokenizer, sep, model_name, device, \n",
        "          classifier, criterion, optimizer, layer=l)\n",
        "    test_loss, test_accuracy = evaluate(test_sentences, test_labels, \n",
        "         model, tokenizer, sep, model_name, device, \n",
        "         classifier, criterion, layer=l)\n",
        "    print(\"layer: {}, test accuracy: {}\".format(l, test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer: 0, test accuracy: 0.8726851851851852\n",
            "layer: 1, test accuracy: 0.9027777777777778\n",
            "layer: 2, test accuracy: 0.9583333333333334\n",
            "layer: 3, test accuracy: 0.9467592592592593\n",
            "layer: 4, test accuracy: 0.9351851851851852\n",
            "layer: 5, test accuracy: 0.9375\n",
            "layer: 6, test accuracy: 0.9398148148148148\n",
            "layer: 7, test accuracy: 0.9375\n",
            "layer: 8, test accuracy: 0.9282407407407407\n",
            "layer: 9, test accuracy: 0.9143518518518519\n",
            "layer: 10, test accuracy: 0.9074074074074074\n",
            "layer: 11, test accuracy: 0.9050925925925926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqkxu9xq3iWB",
        "colab_type": "text"
      },
      "source": [
        "# Experiment 3: Non-linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sML_JwTh3iWC",
        "colab_type": "code",
        "outputId": "fad24d96-bf17-4fd6-d986-89a377c5e9b6",
        "colab": {}
      },
      "source": [
        "num_layers = 12\n",
        "for l in range(num_layers):\n",
        "    classifier, criterion, optimizer = build_classifier(emb_dim, num_labels, nonlinear=True)\n",
        "    train_loss, train_accuracy = train(2, train_sentences, train_labels, \n",
        "          model, tokenizer, sep, model_name, device, \n",
        "          classifier, criterion, optimizer, layer=l)\n",
        "    test_loss, test_accuracy = evaluate(test_sentences, test_labels, \n",
        "         model, tokenizer, sep, model_name, device, \n",
        "         classifier, criterion, layer=l)\n",
        "    print(\"layer: {}, test accuracy: {}\".format(l, test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer: 0, test accuracy: 0.9004629629629629\n",
            "layer: 1, test accuracy: 0.8518518518518519\n",
            "layer: 2, test accuracy: 0.8935185185185185\n",
            "layer: 3, test accuracy: 0.9282407407407407\n",
            "layer: 4, test accuracy: 0.9513888888888888\n",
            "layer: 5, test accuracy: 0.9490740740740741\n",
            "layer: 6, test accuracy: 0.9513888888888888\n",
            "layer: 7, test accuracy: 0.9699074074074074\n",
            "layer: 8, test accuracy: 0.9699074074074074\n",
            "layer: 9, test accuracy: 0.9722222222222222\n",
            "layer: 10, test accuracy: 0.9328703703703703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wiAVNq0G4ei",
        "colab_type": "text"
      },
      "source": [
        "# Experiment 4: Control labels\n",
        "\n",
        "In this experiment we test to see how much of the good performance from Experiments 2 and 3 actually come from things the POS model learned, and how much of it just comes from the probe model. To test this, we use a method from Hewitt and Liang (https://arxiv.org/pdf/1909.03368.pdf). We make a <i>control task</i> which is unrelated to the POS task and do the same probing procedure on the control task. We then measure the <i>selectivity</i> of layers; the difference between their probed accuracy on the POS task and on the control task. If a layer has learned substantial things about the POS task in particular, it should be much better at the POS task than the control task; i.e. it should have high selectivity.\n",
        "\n",
        "Following Hewitt and Liang, we use the following control task for POS tagging. Each word identity will be assigned a random POS tag, with the distribution of POS tags weighted according to their actual appearance. Each word identity will always have the same tag every time it appears. We then train and test the layers on predicting this tag from the embedding. Note that this tag is a deterministic function of the word identity, so high selectivity means the embedding actually has forgotten something about the word identity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S30miOQ3iWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "vocabulary = set(\n",
        "    word\n",
        "      for sentence in (train_sentences + test_sentences)\n",
        "      for word in sentence\n",
        ")\n",
        "all_labels = sum((x.tolist() for x in train_labels), [])\n",
        "control_map = {word: random.choice(all_labels) for word in vocabulary}\n",
        "\n",
        "control_train_labels = [torch.tensor([control_map[word] for word in sentence]) for sentence in train_sentences]\n",
        "control_test_labels = [torch.tensor([control_map[word] for word in sentence]) for sentence in test_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZukN_X9k9_z4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "881943b2-3ddf-47d5-8b02-2f3da795d066"
      },
      "source": [
        "num_layers = 12\n",
        "for l in range(num_layers):\n",
        "    classifier, criterion, optimizer = build_classifier(emb_dim, num_labels, nonlinear=True)\n",
        "    train_loss, train_accuracy = train(2, train_sentences, control_train_labels, \n",
        "          model, tokenizer, sep, model_name, device, \n",
        "          classifier, criterion, optimizer, layer=l)\n",
        "    test_loss, test_accuracy = evaluate(test_sentences, control_test_labels, \n",
        "         model, tokenizer, sep, model_name, device, \n",
        "         classifier, criterion, layer=l)\n",
        "    print(\"layer: {}, test accuracy: {}\".format(l, test_accuracy))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer: 0, test accuracy: 0.35648148148148145\n",
            "layer: 1, test accuracy: 0.4050925925925926\n",
            "layer: 2, test accuracy: 0.44212962962962965\n",
            "layer: 3, test accuracy: 0.4699074074074074\n",
            "layer: 4, test accuracy: 0.4837962962962963\n",
            "layer: 5, test accuracy: 0.5\n",
            "layer: 6, test accuracy: 0.48842592592592593\n",
            "layer: 7, test accuracy: 0.5162037037037037\n",
            "layer: 8, test accuracy: 0.4976851851851852\n",
            "layer: 9, test accuracy: 0.5046296296296297\n",
            "layer: 10, test accuracy: 0.4699074074074074\n",
            "layer: 11, test accuracy: 0.4375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
