{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run for google colab\n",
    "!pip install ninja >> install.log\n",
    "!git clone https://github.com/SIDN-IAP/global-model-repr.git tutorial_code 2>> install.log\n",
    "import sys, torch\n",
    "sys.path.append('/content/tutorial_code')\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Change runtime type to include a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Dissection\n",
    "\n",
    "Our mission: look inside a GAN generator to see what it does.\n",
    "\n",
    "We begin with some imports and jupyter setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running pytorch', torch.__version__, 'using', device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating a pretrained GAN generator.\n",
    "\n",
    "We're going to use a progressive GAN.\n",
    "\n",
    "Below I download and instantiate a model for outdoor churches.\n",
    "\n",
    "You can uncomment the model of your choice.\n",
    "\n",
    "After we create the model, I just print out all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.hub\n",
    "from netdissect import nethook, proggan\n",
    "\n",
    "# n = 'proggan_bedroom-d8a89ff1.pth'\n",
    "n = 'proggan_churchoutdoor-7e701dd5.pth'\n",
    "# n = 'proggan_conferenceroom-21e85882.pth'\n",
    "# n = 'proggan_diningroom-3aa0ab80.pth'\n",
    "# n = 'proggan_kitchen-67f1e16c.pth'\n",
    "# n = 'proggan_livingroom-5ef336dd.pth'\n",
    "# n = 'proggan_restaurant-b8578299.pth'\n",
    "\n",
    "url = 'http://gandissect.csail.mit.edu/models/' + n\n",
    "try:\n",
    "    sd = torch.hub.load_state_dict_from_url(url) # pytorch 1.1\n",
    "except:\n",
    "    sd = torch.hub.model_zoo.load_url(url) # pytorch 1.0\n",
    "model = proggan.from_state_dict(sd).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model.\n",
    "\n",
    "The GAN generator is just a function z->x that transforms random z to realistic images x.\n",
    "\n",
    "To generate images, all we need is a source of random z.  Let's make a micro dataset with a few random z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import zdataset\n",
    "zds = zdataset.z_dataset_for_model(model, size=30, seed=5555)\n",
    "len(zds), zds[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just invoke model(z[None,...]) to generate a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the output data - print a few pixel values\n",
    "model(zds[0][0][None,...].to(device))[0,0,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the output.\n",
    "\n",
    "The netdissect toolkit comes with a few simple visualization tools for examining images in notebooks.\n",
    "\n",
    "  * renormalize turns tensors that were normalized as [-1...1] back into PIL images.\n",
    "  * show takes nested arrays of images and text and lays then out as grids and tables.\n",
    "  \n",
    "Let's look at the images we created with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize, show\n",
    "# from IPython.display import display\n",
    "\n",
    "show([\n",
    "    [renormalize.as_image(model(z[None,...].to(device))[0])]\n",
    "    for [z] in zds\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooking a model with InstrumentedModel\n",
    "\n",
    "To analyze what a model is doing inside, we can wrap it with an InstrumentedModel, which makes it easy to hook or modify a particular layer.\n",
    "\n",
    "InstrumentedModel adds a few useful functions for inspecting a model, including:\n",
    "   * `model.retain_layer('layername')` - hooks a layer to hold on to its output after computation\n",
    "   * `model.retained_layer('layername')` - returns the retained data from the last computation\n",
    "   * `model.edit_layer('layername', rule=...)` - runs the `rule` function after the given layer\n",
    "   * `model.remove_edits()` - removes editing rules\n",
    "\n",
    "Let's setup `retain_layer` now.  We'll pick a layer sort of in the early-middle of the generator.  You can pick whatever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add a summary of what InstrumentedModel can do.\n",
    "# retain a layer, get a retined layer, edit a layer\n",
    "\n",
    "from netdissect import nethook\n",
    "\n",
    "# Don't re-wrap it, if it's already wrapped (e.g., if you press enter twice)\n",
    "if not isinstance(model, nethook.InstrumentedModel):\n",
    "    model = nethook.InstrumentedModel(model)\n",
    "model.retain_layer('layer4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the model and inspect the internal units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "img = model(zds[0][0][None,...].to(device))\n",
    "\n",
    "# As a side-effect, the model has retained the output of layer4.\n",
    "acts = model.retained_layer('layer4')\n",
    "\n",
    "# We can look at it.  How much data is it?\n",
    "acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just look at the 0th convolutional channel.\n",
    "print(acts[0,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activation data\n",
    "\n",
    "It can be informative to visualize activation data instead of just looking at the numbers.\n",
    "\n",
    "Net dissection comes with an ImageVisualizer object for visualizing grid data as an image in a few different ways.  Here is a heatmap of the array above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "iv = imgviz.ImageVisualizer(100)\n",
    "iv.heatmap(acts[0,1], mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you tell IV to dereference the activations for you, it scales heatmaps according to global statistics.\n",
    "\n",
    "What is happening with unit 418?\n",
    "\n",
    "Each unit has a different scale, which makes the heatmaps harder to interpret.\n",
    "\n",
    "We can normalize the scales by collecting stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\n",
    "    [['unit %d' % u,\n",
    "      [iv.image(img[0])],\n",
    "      [iv.masked_image(img[0], acts, (0,u))],\n",
    "      [iv.heatmap(acts, (0,u), mode='nearest')],\n",
    "     ] for u in range(414, 420)]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting quantile statistics for every unit\n",
    "\n",
    "We want to know per-channel minimum or maximum values, means, medians, quantiles, etc.\n",
    "\n",
    "We want to treat each pixel as its own sample for all the channels.  For example, here are the activations for one image as an 8x8 tensor over with 512 channels.  We can disregard the geometry and just look at it as a 64x512 sample matrix, that is 64 samples of 512-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acts.shape)\n",
    "print(acts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net dissection has a tally package that tracks quantiles over large samples.\n",
    "\n",
    "To use it, just define a function that returns sample matrices like the 64x512 above, and then it will call your function on every batch and tally up the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "\n",
    "# To collect stats, define a function that returns 2d [samples, units]\n",
    "def compute_samples(zbatch):\n",
    "    _ = model(zbatch.to(device))          # run the model\n",
    "    acts = model.retained_layer('layer4') # get the activations, and flatten\n",
    "    return acts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "\n",
    "# Then tally_quantile will run your function over the whole dataset to collect quantile stats\n",
    "rq = tally.tally_quantile(compute_samples, zds)\n",
    "\n",
    "# Print out the median value for the first 20 channels\n",
    "rq.quantiles(0.5)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(3).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring quantiles\n",
    "\n",
    "The rq object tracks a sketch of all the quantiles of the sampled data.  For example, what is the mean, median, and percentile value for each unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells me now, for example, what the means are for channel,\n",
    "# rq.mean()\n",
    "# what median is,\n",
    "# rq.quantiles(0.5)\n",
    "# Or what the 99th percentile quantile is.\n",
    "# rq.quantiles(0.99)\n",
    "\n",
    "(rq.quantiles(0.8) > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantiles can be plugged directly into the ImageVisualizer to put heatmaps on an informative per-unit scale.  When you do this:\n",
    "\n",
    "   * Heatmaps are shown on a scale from black to white from 1% lowest to the 99% highest value.\n",
    "   * Masked image lassos are shown at a 95% percentile level (by default, can be changed).\n",
    "   \n",
    "Now unit 418 doesn't drown out the other ones in the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iv = imgviz.ImageVisualizer(100, quantiles=rq)\n",
    "show([\n",
    "    [  # for every unit, make a block containing\n",
    "       'unit %d' % u,         # the unit number\n",
    "       [iv.image(img[0])],    # the unmodified image\n",
    "       [iv.masked_image(img[0], acts, (0,u))], # the masked image\n",
    "       [iv.heatmap(acts, (0,u), mode='nearest')], # the heatmap\n",
    "    ]\n",
    "    for u in range(414, 420)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing top-activating images\n",
    "\n",
    "A useful way to visualize units is to sort a sample, in order of highest activation.  tally_topk does this.\n",
    "\n",
    "Like torch.topk, it returns both the top k values and the top k indexes.  But instead of acting on a single tensor, it iterates over the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_max(zbatch):\n",
    "    image_batch = model(zbatch.to(device))\n",
    "    return model.retained_layer('layer4').max(3)[0].max(2)[0]\n",
    "\n",
    "topk = tally.tally_topk(compute_image_max, zds)\n",
    "topk.result()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unit, this function prints out unit masks from the top-activating images\n",
    "def unit_viz_row(unitnum, percent_level=None):\n",
    "    out = []\n",
    "    for imgnum in topk.result()[1][unitnum][:8]:\n",
    "        img = model(zds[imgnum][0][None,...].to(device))\n",
    "        acts = model.retained_layer('layer4')\n",
    "        out.append([imgnum.item(),\n",
    "                    # [iv.image(img[0])],\n",
    "                    [iv.masked_image(img[0], acts, (0, unitnum), percent_level=percent_level)],\n",
    "                    # [iv.heatmap(acts, (0, unitnum), mode='nearest')],\n",
    "                   ])\n",
    "    return out\n",
    "\n",
    "show(unit_viz_row(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling semantics within the generated images\n",
    "\n",
    "Let's quantify what's inside these images by segmenting them.\n",
    "\n",
    "First, we create a segmenter network.  (We use the Unified Perceptual Parsing segmenter by Xiao, et al. (https://arxiv.org/abs/1807.10221).\n",
    "\n",
    "Note that the segmenter we use here requires a GPU.\n",
    "\n",
    "If you have a CPU only, you can skip to step \"Examining units that select for trees\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import segmenter\n",
    "\n",
    "segmodel = segmenter.UnifiedParsingSegmenter(segsizes=[256])\n",
    "seglabels = [l for l, c in segmodel.get_label_and_category_names()[0]]\n",
    "print('segmenter has', len(seglabels), 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create segmentation images for the dataset.  Here tally_cat just concatenates batches of image (or segmentation) data.\n",
    "\n",
    "  * `segmodel.segment_batch` segments an image\n",
    "  * `iv.segmentation(seg)` creates a solid-color visualization of a segmentation\n",
    "  * `iv.segment_key(seg, segmodel)` makes a small legend for the segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from netdissect import upsample\n",
    "from netdissect import segviz\n",
    "\n",
    "imgs = tally.tally_cat(lambda zbatch: model(zbatch.to(device)), zds)\n",
    "seg = tally.tally_cat(lambda img: segmodel.segment_batch(img.cuda(), downsample=1), imgs)\n",
    "\n",
    "from netdissect.segviz import seg_as_image, segment_key\n",
    "show([\n",
    "    (iv.image(imgs[i]),\n",
    "     iv.segmentation(seg[i,0]),\n",
    "     iv.segment_key(seg[i,0], segmodel)\n",
    "    )\n",
    "    for i in range(len(seg))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying units that correlate with visual concepts\n",
    "\n",
    "Some units align well with visual concepts.\n",
    "\n",
    "To identify these, we will collect *conditional* activation statistics.\n",
    "\n",
    "In addition to regular quantile statistics, we will collect quantile statistics over all the subsets of pixels in which a particular visual concept is present.\n",
    "\n",
    "To do this, we will use the `tally_conditional_quantile` loop.\n",
    "\n",
    "It expects its `compute` function to return a list of sample statistics, each one keyed by a condition that is present.\n",
    "\n",
    "Here is how we do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upsample activations to measure them at each segmentation location.\n",
    "upfn = upsample.upsampler((64, 64), (8, 8))\n",
    "\n",
    "def compute_conditional_samples(zbatch):\n",
    "    image_batch = model(zbatch.to(device))\n",
    "    seg = segmodel.segment_batch(image_batch, downsample=4)\n",
    "    upsampled_acts = upfn(model.retained_layer('layer4'))\n",
    "    return tally.conditional_samples(upsampled_acts, seg)\n",
    "\n",
    "# Run this function once to sample one image\n",
    "sample = compute_conditional_samples(zds[0][0].cuda()[None,...])\n",
    "\n",
    "# The result is a list of all the conditional subsamples\n",
    "[(c, d.shape) for c, d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cq = tally.tally_conditional_quantile(compute_conditional_samples, zds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional quantile statistics let us compute lots of relationships between units and visual concepts.\n",
    "\n",
    "For example, IoU is the \"inside over outside\" ratio, measuring how much overlap there is between the top few percent activations of a unit and the presence of a visual concept.  We can estimate the IoU ratio for all pairs between units and concepts with these stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_table = tally.iqr_from_conditional_quantile(cq, cutoff=0.95)\n",
    "iou_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view a few of the units, labeled with an associated concept, sorted from highest to lowest IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_list = sorted(enumerate(zip(*iou_table.max(1))), key=lambda k: -k[1][0])\n",
    "\n",
    "for unit, (iou, segc) in unit_list[:5]:\n",
    "    print('unit %d: %s (iou %.2f)' % (unit, seglabels[segc], iou))\n",
    "    show(unit_viz_row(unit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining units that select for trees\n",
    "\n",
    "Now let's filter just units that were labeled as 'tree' units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_units = [(unit, iou, segc) for unit, (iou, segc) in unit_list if seglabels[segc] == 'tree'][:10]\n",
    "# If you can't run the segmenter, uncomment the line below and comment the one above.\n",
    "# tree_units = [365, 157, 119, 374, 336, 195, 278, 76, 408, 125]\n",
    "\n",
    "for unit, iou, segc in tree_units:\n",
    "    print('unit %d, iou %.2f' % (unit, iou))\n",
    "    show(unit_viz_row(unit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing a model by altering units\n",
    "\n",
    "Now let's try changing some units directly to see what they do.\n",
    "\n",
    "We we will use `model.edit_layer` to do that.\n",
    "\n",
    "This works by just allowing you to define a function that edits the output of a layer.\n",
    "\n",
    "We will edit the output of `layer4` by zeroing ten of the tree units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zero_out_tree_units(data, model):\n",
    "    data[:, tree_units, :, :] = 0\n",
    "    return data\n",
    "\n",
    "model.edit_layer('layer4', rule=zero_out_tree_units)\n",
    "edited_imgs = tally.tally_cat(lambda zbatch: model(zbatch.to(device)), zds)\n",
    "show([(['Before', [renormalize.as_image(imgs[i])]],\n",
    "       ['After', [renormalize.as_image(edited_imgs[i])]]) for i in range(len(zds))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional experiments\n",
    "\n",
    "Now try the following experiments:\n",
    "   * Instead of zeroing the units, try setting them negative, e.g., to -2.\n",
    "   * Instead of turning the units off, try turning them on, e.g., set them to 10.\n",
    "   * Try turning on the units in just the left-half of the images.\n",
    "   * Try altering other sets of units.\n",
    "It is not hard to create a 'painting' program using these effects.  To demo one, visit http://gandissect.csail.mit.edu/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing cutoff thresholds to maximize relative mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.remove_edits()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last visualization - just showing masks and units side-by-side for a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iv2 = imgviz.ImageVisualizer(256, quantiles=rq)\n",
    "model.remove_edits()\n",
    "def compute_acts(zbatch):\n",
    "    model(zbatch.to(device))\n",
    "    return model.retained_layer('layer4')\n",
    "\n",
    "acts = tally.tally_cat(compute_acts, zds).cpu()\n",
    "unit = 365\n",
    "segc = 4\n",
    "\n",
    "if True:\n",
    "    show([\n",
    "        (iv2.image(imgs[i]),\n",
    "         iv2.segmentation(seg[i,0], 4),\n",
    "         iv2.heatmap(acts, (i, unit), mode='nearest'),\n",
    "         iv2.masked_image(imgs[i], acts, (i, unit))\n",
    "        )\n",
    "        for i in [0,2,4,9,25,28]\n",
    "    ])\n",
    "    \n",
    "if False:\n",
    "    show([\n",
    "        (iv2.image(imgs[i]),\n",
    "         iv2.segmentation(seg[i,0], 4),\n",
    "         iv2.heatmap(acts, (i, unit), mode='nearest'),\n",
    "         iv2.masked_image(imgs[i], acts, (i, unit))\n",
    "        )\n",
    "        for i in [1,5,7,8,10,11]\n",
    "    ])\n",
    "\n",
    "if False:\n",
    "    show([\n",
    "        (iv2.image(imgs[i]),\n",
    "         iv2.segmentation(seg[i,0], 11),\n",
    "         iv2.heatmap(acts, (i, 149), mode='nearest'),\n",
    "         iv2.masked_image(imgs[i], acts, (i, 149))\n",
    "        )\n",
    "        for i in [0,2,4,9,25,28]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail - optimizing cutoff threshold\n",
    "\n",
    "We can do better if we try to remove bias having to do with the cutoff threshold.\n",
    "\n",
    "In the above, we evaluate the unit by applying a cutoff at a fixed 95% percentile, which biases our search for matching visual concepts to favor of objects that take about 5% of pixels on average, such as trees.\n",
    "\n",
    "To avoid this bias, we can choose to look at each (unit, concept) pair at the best possible unit threshold for that pair.  Bellow, we identify a \"maximally informative\" threshold - that is, we find a threshold for the unit that maximizes the relative mutual information between the unit and the concept.   IQR stands for \"information quatlity ratio\", and it's mutual information divided by joint entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "cutoffs = 1 - torch.logspace(-3, math.log10(0.5), 50)\n",
    "iqr_grid = tally.iqr_from_conditional_quantile(cq, cutoff=cutoffs, min_batches=3)\n",
    "print(iqr_grid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is small enough data that we can find levels that maximize iqr by just doing a simple grid search. Here is an example of finding a cutoff that maximizes the overlap between unit 3 and concept 340."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cutoffs.numpy(), iqr_grid[3,340].numpy())\n",
    "# plt.figure()\n",
    "# plt.plot(cutoffs.numpy(), iou_table[3,340].numpy())\n",
    "maxval, maxindex = iqr_grid[3,340].max(0)\n",
    "print(maxval, cutoffs[maxindex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we collect IoU measures at each of the best cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_cutoff = cutoffs[iqr_grid.max(2)[1]]\n",
    "iou_grid = tally.iou_from_conditional_quantile(cq, cutoff=cutoffs)\n",
    "best_iou = iqr_grid.gather(2, iqr_grid.max(2)[1][...,None])[...,0]\n",
    "best_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the units that best match concepts based on these optimal cutoffs.\n",
    "\n",
    "Note that this procedure has allowed us to identify both units for very common general concepts like \"sky\" (at threshold 73.5%) and very rare narrow concepts like \"door\" (at threshold 99.6%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iv = imgviz.ImageVisualizer(100, quantiles=cq.conditional(0))\n",
    "best_unit_list = sorted(enumerate(zip(*best_iou.max(1))), key=lambda k: -k[1][0])\n",
    "for unit, (iou, segc) in best_unit_list[:50]:\n",
    "    print('unit %d: %s (iou %.2f at cutoff %.1f%%)' % (unit, seglabels[segc], iou, 100*best_cutoff[unit, segc]))\n",
    "    show(unit_viz_row(unit, percent_level=best_cutoff[unit, segc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special thanks to Lucy Chai, \u00c0gata Lapedriza, and Katherine Gallagher for testing this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}